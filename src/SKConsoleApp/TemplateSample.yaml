# The name of the function, used by default when creating prompt functions.
name: ExampleFunction

# A brief description of what this function does.
description: An example prompt function using all available YAML attributes for Semantic Kernel.

# The format of the template, which defines how the prompt is structured.
# Supported formats: semantic-kernel, handlebars, liquid.
template_format: semantic-kernel

# The main template string that defines how the prompt is constructed.
template: |
  Generate a detailed response using the topic {{topic}} and the context "{{context}}".

# A list of input variables required by the prompt template.
input_variables:
  - name: topic
    # A description of what this variable represents.
    description: The primary topic for generating the response.
    # The default value used if no input is provided.
    default: "technology"
    # Specifies whether this variable is mandatory.
    is_required: true
    # Optional JSON schema describing the variable's structure.
    json_schema: 
      type: string
      minLength: 1
      maxLength: 50
    # Controls if the variable can include potentially dangerous content.
    allow_dangerously_set_content: false

  - name: context
    description: Additional context to be used in the response.
    default: "general overview"
    is_required: false
    json_schema: 
      type: string
      minLength: 0
      maxLength: 200
    allow_dangerously_set_content: false

# The output variable where the generated content will be stored.
output_variable:
  # A description of the output produced by the function.
  description: The output generated by the AI model.
  # JSON schema describing the structure and type of the output.
  json_schema: 
    type: string

# Configuration settings for how the prompt should be executed by different services.
execution_settings:
  # Execution settings for the Azure OpenAI service located in East US.
  azure_openai_eastus:
    # Identifier for the AI service.
    service_id: azure_openai_eastus
    # AI model to be used for this service.
    model_id: gpt-4
    # Determines whether the AI model should call functions automatically.
    function_choice_behavior: auto
    # Temperature setting for controlling randomness in responses.
    temperature: 0.7

  # Execution settings for OpenAI's API using GPT-3.5-turbo.
  openai:
    service_id: openai
    model_id: gpt-3.5-turbo
    function_choice_behavior: required
    temperature: 0.5

  # Default execution settings when no specific service is specified.
  default:
    service_id: default
    model_id: gpt-3.5
    function_choice_behavior: none
    temperature: 0.6

# Global setting controlling whether potentially dangerous content is allowed.
allow_dangerously_set_content: false
